# Descripción del Proyecto

Este proyecto contiene varios scripts y archivos de datos relacionados con la generación de datos, limpieza, extracción de características y análisis. A continuación se presenta una breve descripción de cada archivo:

## Archivos y Scripts

### data_cleaning.py
- Este script se encarga de limpiar el conjunto de datos. Maneja los valores faltantes y realiza las transformaciones necesarias para preparar los datos para el análisis o modelado.

### data_consumer.py
- Este script probablemente está destinado a consumir o leer datos, posiblemente de un flujo de Kafka u otra fuente de datos similar, y procesarlos o almacenarlos en una base de datos.

### data_producer.py
- Este script probablemente está destinado a producir o enviar datos, posiblemente a un flujo de Kafka u otra fuente de datos similar, para su consumo por otros servicios o aplicaciones.

### dummy_kafka_data.csv
- Este es el conjunto de datos original generado para el proyecto. Contiene varios campos, incluidos user_id, item_id, rating, timestamp, location y comments.

### EDA.ipynb
- Este Jupyter Notebook contiene análisis exploratorio de datos (EDA) en el conjunto de datos. Incluye visualizaciones y estadísticas para comprender mejor los datos.

### feature_extraction.py
- Este script realiza la extracción de características en el conjunto de datos. Tokeniza los comentarios, extrae nuevas características como la longitud del comentario y detalles de la marca de tiempo, y normaliza ciertas columnas.

### featured_dummy_kafka_data.csv
- Este conjunto de datos es una versión intermedia de los datos procesados. Incluye los datos originales con características adicionales extraídas, pero puede no estar completamente limpio o normalizado.

### generate_dummy_data.py
- Este script genera los datos iniciales ficticios para el proyecto. Crea un conjunto de datos con varios campos, incluidos timestamps, locations y comments aleatorios.

### kafka_data.csv
- Este parece ser un conjunto de datos adicional o alternativo utilizado en el proyecto. Puede contener datos similares o suplementarios a los de `dummy_kafka_data.csv`.

## Primeros Pasos

1. **Generación de Datos**: Ejecuta `generate_dummy_data.py` para crear el conjunto de datos inicial.
2. **Limpieza de Datos**: Utiliza `data_cleaning.py` para limpiar y preprocesar el conjunto de datos.
3. **Extracción de Características**: Aplica `feature_extraction.py` para extraer características adicionales y normalizar los datos.
4. **Análisis Exploratorio de Datos**: Abre `EDA.ipynb` en Jupyter Notebook para realizar EDA en el conjunto de datos limpio.
5. **Consumo/Producción de Datos**: Utiliza `data_consumer.py` y `data_producer.py` para necesidades de transmisión de datos o procesamiento adicional.

## Dependencias

- Python 3.x
- pandas
- numpy
- nltk
- matplotlib
- seaborn
- wordcloud
- sklearn

Asegúrate de instalar los paquetes de Python necesarios usando `pip install -r requirements.txt` si se proporciona un archivo de requisitos.

## Notas

- Ajusta las rutas de los archivos y los parámetros según sea necesario para que coincidan con tu entorno específico y datos.
- Asegúrate de que los archivos de datos necesarios estén presentes en el directorio antes de ejecutar los scripts.

Para más detalles o problemas, consulta los scripts individuales o contacta al mantenedor del proyecto.
